{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_E0ZBJmAeytU"
   },
   "outputs": [],
   "source": [
    "# NOTE: If you want to use a GPU, uncomment the below lines\n",
    "# from tensorflow.python.client import device_lib\n",
    "# assert 'GPU' in str(device_lib.list_local_devices())          # confirm TensorFlow sees the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L10RytSHezMj"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "import IPython\n",
    "\n",
    "from tensorflow.python.keras.preprocessing import image as pre\n",
    "from tensorflow.python.keras import models \n",
    "from tensorflow.python.keras import backend as K\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RpQzI93ee0kc"
   },
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()\n",
    "\n",
    "content_path = '../content/turtle.jpg'\n",
    "style_path = '../style/picasso.jpg'\n",
    "\n",
    "ARTWORK_FOLDER = \"../style\"\n",
    "PICTURE_FOLDER = \"../content\"\n",
    "OUTPUT_FOLDER = \"../output\"\n",
    "\n",
    "MAX_DIM = 512\n",
    "\n",
    "# For de-processing processed images\n",
    "CHANNEL_MEANS = np.array([103.939, 116.779, 123.68])\n",
    "\n",
    "content_layers = ['block5_conv2']\n",
    "style_layers = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1']\n",
    "\n",
    "num_content_layers = len(content_layers)\n",
    "num_style_layers = len(style_layers)\n",
    "\n",
    "UPDATE_EPOCH = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OGlS0Ufae2G8"
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#\n",
    "# We would like to acknowledge a source that helped us process and deprocess\n",
    "# images for the purpose of this assignment. This includes using eager execution\n",
    "# in tensorflow, loading, preparing, and deprocessing and saving the images as\n",
    "# they traverse through the neural network and the custom optimizer.\n",
    "#\n",
    "# Acknowledgements:\n",
    "# https://medium.com/tensorflow/neural-style-transfer-creating-art-with-deep-learning-using-tf-keras-and-eager-execution-7d541ac31398\n",
    "#\n",
    "################################################################################\n",
    "\n",
    "def load_image(image_file):\n",
    "    ''' Function that loads an image from an input file path.\n",
    "    The function will return a numpy array that represents the\n",
    "    pixel values of the image.'''\n",
    "    \n",
    "    # Get the image and a scaling factor\n",
    "    image = Image.open(image_file)\n",
    "    image_scale = MAX_DIM / max(image.size)\n",
    "\n",
    "    # Resize the image according to the scale and get it in array form\n",
    "    image = image.resize((round(image.size[0] * image_scale), round(image.size[1] * image_scale)), Image.ANTIALIAS)\n",
    "    image = pre.img_to_array(image)\n",
    "\n",
    "    # Add a dimension in the 0th axis to account for the batch size\n",
    "    image = np.expand_dims(image, axis = 0)\n",
    "    \n",
    "    return image\n",
    "\n",
    "\n",
    "def deprocess_image(processed_image):\n",
    "    ''' Function that de-processes the image after it finishes\n",
    "    a forward pass of the neural model. This processing includes\n",
    "    shifting the pixel values by the normalized mean and\n",
    "    removing the \"batch\" dimension. '''\n",
    "\n",
    "    # remove the \"batch\" dimension from the image\n",
    "    image = processed_image.copy()\n",
    "    if len(image.shape) == 4:\n",
    "        image = np.squeeze(image, axis = 0)\n",
    "\n",
    "    # This reverses the process of tf.keras.applications.vgg19.preprocess_input\n",
    "    image[:, :, 0] += CHANNEL_MEANS[0]\n",
    "    image[:, :, 1] += CHANNEL_MEANS[1]\n",
    "    image[:, :, 2] += CHANNEL_MEANS[2]\n",
    "    image = image[:, :, ::-1]\n",
    "\n",
    "    # ensure pixel values are within \n",
    "    image = np.clip(image, 0, 255).astype('uint8')\n",
    "    return image\n",
    "\n",
    "def imshow(image):\n",
    "\n",
    "    # Remove the batch dimension\n",
    "    image = np.squeeze(image, axis = 0)\n",
    "    plt.imshow(image.astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NdYynCZre4gT"
   },
   "outputs": [],
   "source": [
    "def model():\n",
    "    ''' Function that returns a neural network model derived from the Imagenet\n",
    "    VGG19 model. The inputs of the model are the same as that of the VGG19\n",
    "    model, however, the outputs are the feature representations of the input\n",
    "    at the specified layers in config.py. '''\n",
    "\n",
    "    # get the original model\n",
    "    original_vgg = tf.keras.applications.vgg19.VGG19(include_top = False, weights = 'imagenet')\n",
    "\n",
    "    # specify the outputs\n",
    "    content_outputs = [original_vgg.get_layer(layer).output for layer in content_layers]\n",
    "    style_outputs = [original_vgg.get_layer(layer).output for layer in style_layers]\n",
    "\n",
    "    # return the custom model\n",
    "    return models.Model(original_vgg.input, content_outputs + style_outputs)\n",
    "\n",
    "\n",
    "def gram_matrix(feature):\n",
    "    ''' Based on the implementation described in\n",
    "    https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf\n",
    "    '''\n",
    "    # flatten the features to one column per channel\n",
    "    num_channels = feature.shape[-1]\n",
    "    feature = tf.reshape(feature, (-1, num_channels))\n",
    "\n",
    "    return tf.matmul(feature, feature, transpose_a = True)\n",
    "\n",
    "\n",
    "def gram_style_loss(predicted_gram, actual_gram, pixels, channels):\n",
    "    ''' Based on the implementation described in\n",
    "    https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf\n",
    "    '''\n",
    "    loss_ = tf.reduce_sum(tf.square(predicted_gram - actual_gram))\n",
    "    return loss_ / (4 * (pixels ** 2) * (channels ** 2))\n",
    "\n",
    "\n",
    "def content_loss(predicted, actual):\n",
    "    ''' Based on the implementation described in\n",
    "    https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf\n",
    "    '''\n",
    "    return 0.5 * tf.reduce_sum(tf.square(predicted - actual))\n",
    "\n",
    "\n",
    "def loss(model, output_image, actual_content_features, actual_gram_features, alpha = 10000, beta = 1):\n",
    "    ''' Function that computes the total loss for the current iteration of the gradient descent\n",
    "    algorithm. The loss function is based on the implementation described in\n",
    "    https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf\n",
    "    The function returns a linear combination of the content and style loss based on the multipliers\n",
    "    (alpha and beta) specified. '''\n",
    "\n",
    "    # get the content and style representations\n",
    "    model_representation = model(output_image)\n",
    "    content_representation = model_representation[ : len(content_layers)]\n",
    "    style_representation = model_representation[len(content_layers) : ]\n",
    "\n",
    "    # initialize loss counters\n",
    "    total_content_loss, total_style_loss = 0, 0\n",
    "\n",
    "    # for every content representation, get the loss\n",
    "    for actual_content, predicted_content in zip(actual_content_features, content_representation):\n",
    "        total_content_loss += content_loss(predicted_content, actual_content)\n",
    "\n",
    "    # for every style representation, get the loss for the corresponding gram matrix\n",
    "    for actual_gram, predicted_style in zip(actual_gram_features, style_representation):\n",
    "        predicted_gram = gram_matrix(predicted_style)\n",
    "        shape = tf.shape(predicted_style)\n",
    "        total_style_loss += gram_style_loss(predicted_gram, actual_gram, float(shape[0] * shape[1]), float(shape[-1]))\n",
    "\n",
    "    # normalize loss per layer\n",
    "    total_content_loss /= len(content_layers)\n",
    "    total_style_loss /= len(style_layers)\n",
    "\n",
    "    # return total loss based on the linear combination\n",
    "    loss_ = (alpha * total_content_loss) + (beta * total_style_loss)\n",
    "    return loss_\n",
    "\n",
    "\n",
    "def feature_representations(model, content_image, style_image):\n",
    "    ''' Function that returns the feature representations of a\n",
    "    content and style image.'''\n",
    "\n",
    "    content = model(content_image)\n",
    "    style = model(style_image)\n",
    "\n",
    "    content_features = [layer for layer in content[ : len(content_layers)]]\n",
    "    style_features = [layer for layer in style[len(content_layers) : ]]\n",
    "\n",
    "    return content_features, style_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tC9JfUJie6U1"
   },
   "outputs": [],
   "source": [
    "def loss_and_gradients(my_model, output_image, content_features, gram_style_features, alpha = 10000, beta = 1):\n",
    "    ''' Function that computes the gradients for the output image based on the actual content and style\n",
    "    features as well as their respective multipliers. '''\n",
    "    with tf.GradientTape() as g:\n",
    "        loss_ = loss(my_model, output_image, content_features, gram_style_features, alpha, beta)\n",
    "    return loss_, g.gradient(loss_, output_image)\n",
    "\n",
    "\n",
    "def transfer_style(content_image, style_image, epochs = 10000, alpha = 10000, beta = 1):\n",
    "    ''' Function that transfers the style from the content_image to the style_image. \n",
    "    The gradient descent algorithm is performed over 'epochs' iterations and the \n",
    "    multiplier for the content and style are specified by alpha and beta respectively. '''\n",
    "\n",
    "    # get the model\n",
    "    my_model = model()\n",
    "    for layer in my_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # get the actual content and style representations\n",
    "    content_features, style_features = feature_representations(my_model, content_image, style_image)\n",
    "    gram_style_features = [gram_matrix(feature) for feature in style_features]\n",
    "\n",
    "    # create a random noise image\n",
    "    output_image = np.random.randint(0, 256, size = content_image.shape).astype('float32')\n",
    "    output_image = tf.keras.applications.vgg19.preprocess_input(output_image)\n",
    "    output_image = tfe.Variable(output_image, dtype=tf.float32)\n",
    "\n",
    "    # create tracking variables and the Adam optimizer\n",
    "    counter, min_loss, best_image = 1, float('inf'), None\n",
    "    optimizer = tf.train.AdamOptimizer(5, beta1=0.99, epsilon=0.1)\n",
    "\n",
    "    # set the minimum and maximum values allowed in the image accounting for channel means\n",
    "    minimum, maximum = - CHANNEL_MEANS, 255 - CHANNEL_MEANS\n",
    "\n",
    "    # start the timer\n",
    "    start_time = time.time()\n",
    "    update_start_time = time.time()\n",
    "\n",
    "    for i in range(epochs):\n",
    "\n",
    "        # get the loss and gradients for the output image based on the current iteration\n",
    "        loss_, curr_gradients = loss_and_gradients(my_model, output_image, content_features, gram_style_features, alpha, beta)\n",
    "\n",
    "        # apply the gradients based on the current loss\n",
    "        optimizer.apply_gradients([(curr_gradients, output_image)])\n",
    "\n",
    "        # clip the new matrix based on the minimum and maximum values\n",
    "        output_image.assign(tf.clip_by_value(output_image, minimum, maximum))\n",
    "\n",
    "        # if a new best image is found\n",
    "        if loss_ < min_loss:\n",
    "            min_loss = loss_\n",
    "            best_image = output_image.numpy()\n",
    "\n",
    "        # print loss and time statistics after every UPDATE_EPOCH iterations\n",
    "        if i % UPDATE_EPOCH == 0:\n",
    "          \n",
    "            plot_img = output_image.numpy()\n",
    "            plot_img = deprocess_image(plot_img)\n",
    "            IPython.display.clear_output(wait=True)\n",
    "            IPython.display.display_png(Image.fromarray(plot_img))\n",
    "\n",
    "            print(\"Iteration: {} \\t Total Loss: {}\".format(i, loss_))\n",
    "            print(\"Time for {} epochs = {} seconds.\".format(UPDATE_EPOCH, time.time() - update_start_time))\n",
    "            update_start_time = time.time()\n",
    "\n",
    "    # print the total time taken to perform the optimization algorithm\n",
    "    print(\"\\nTotal Time: {} seconds\".format(time.time() - start_time))\n",
    "\n",
    "    # return the best found image and the associated overall loss\n",
    "    return deprocess_image(best_image), min_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kkgek5gAe8Fg"
   },
   "outputs": [],
   "source": [
    "def color_transfer(source, target):\n",
    "    # convert color space from BGR to L*a*b color space\n",
    "    # note - OpenCV expects a 32bit float rather than 64bit\n",
    "    source = cv2.cvtColor(source, cv2.COLOR_BGR2RGB).astype(\"float32\")\n",
    "    target = cv2.cvtColor(target, cv2.COLOR_BGR2RGB).astype(\"float32\")\n",
    "    # compute color stats for both images\n",
    "    (smean, scov) = image_stats(source)\n",
    "    (tmean, tcov) = image_stats(target)\n",
    "\n",
    "    # cholesky\n",
    "    L_s = np.linalg.cholesky(scov)\n",
    "    L_t = np.linalg.cholesky(tcov)\n",
    "    Lt_inv = np.linalg.inv(L_t)\n",
    "    A = np.matmul(L_s, Lt_inv)\n",
    "    b_act = smean - (np.matmul(A, tmean))\n",
    "    \n",
    "    (r, g, b) = cv2.split(target)\n",
    "    new_img = target.copy()\n",
    "    \n",
    "    m,n = r.shape\n",
    "    for x in range(m):\n",
    "      for y in range(n):\n",
    "        x_val = np.matrix([[r[x][y], g[x][y], b[x][y]]]).transpose()\n",
    "        x_new = np.matmul(A, x_val) + b_act\n",
    "        new_img[x][y][0] = x_new[0][0]\n",
    "        new_img[x][y][1] = x_new[1][0]\n",
    "        new_img[x][y][2] = x_new[2][0]\n",
    "    \n",
    "    \n",
    "    # converting back to BGR\n",
    "    transfer = cv2.cvtColor(new_img.astype(\"uint8\"), cv2.COLOR_RGB2BGR)\n",
    "    return transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ltckV2-Le91-"
   },
   "outputs": [],
   "source": [
    "def image_stats(image):\n",
    "    # compute mean and standard deviation of each channel\n",
    "    (r, g, b) = cv2.split(image)\n",
    "    \n",
    "    rMean = np.average(r)\n",
    "    gMean = np.average(g)\n",
    "    bMean = np.average(b)\n",
    "    mean = np.matrix([[rMean, gMean, bMean]]).transpose()\n",
    "    cov = np.matrix([[0.0, 0.0, 0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]])\n",
    "    m,n = r.shape\n",
    "    for x in range(m):\n",
    "      for y in range(n):\n",
    "        x_val = np.matrix([[r[x][y], g[x][y], b[x][y]]]).transpose()\n",
    "        temp_one = (x_val - mean)\n",
    "        temp_two = np.matrix(temp_one).transpose()\n",
    "        sum_val = ((np.matmul(temp_one, temp_two)) / (m*n))\n",
    "        cov += sum_val\n",
    "\n",
    "    return (mean, cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mdzCqYNbe_e9"
   },
   "outputs": [],
   "source": [
    "def show_image(title, image, width=720):\n",
    "    r = width/float(image.shape[1])\n",
    "    dim = (width, int(image.shape[0]*r))\n",
    "    resized = cv2.resize(image, dim, interpolation=cv2.INTER_AREA)\n",
    "    print(\"show images\")\n",
    "    cv2.imshow(title, resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1324
    },
    "colab_type": "code",
    "id": "pZthfLyPfBIM",
    "outputId": "3e2e5e5e-b3ad-4451-cf06-a3d2cf4ade93"
   },
   "outputs": [],
   "source": [
    "target = cv2.imread(style_path)\n",
    "source = cv2.imread(content_path)\n",
    "\n",
    "# transfer of color\n",
    "transfer = color_transfer(source, target)\n",
    "\n",
    "# display of image\n",
    "img = plt.imshow(cv2.cvtColor(source, cv2.COLOR_BGR2RGB)); \n",
    "img.set_cmap('hot')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "img = plt.imshow(cv2.cvtColor(target, cv2.COLOR_BGR2RGB)); \n",
    "img.set_cmap('hot')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "img = plt.imshow(cv2.cvtColor(transfer, cv2.COLOR_BGR2RGB)); \n",
    "img.set_cmap('hot')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "im = Image.fromarray(transfer)\n",
    "new_style_path = '../output/transfer.jpg'\n",
    "im.save(new_style_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 427
    },
    "colab_type": "code",
    "id": "O_LN1aZwfCsj",
    "outputId": "3029d825-ff6f-4dd8-fa8f-a7fbb64cd7d6"
   },
   "outputs": [],
   "source": [
    "content_image = load_image(content_path)\n",
    "style_image = load_image(new_style_path)\n",
    "\n",
    "content_image = tf.keras.applications.vgg19.preprocess_input(content_image).astype('float32')\n",
    "style_image = tf.keras.applications.vgg19.preprocess_input(style_image).astype('float32')\n",
    "\n",
    "print(\"Loaded the images...\")\n",
    "print(\"Starting style transfer...\")\n",
    "output_image, mis_loss = transfer_style(content_image, style_image)\n",
    "im = Image.fromarray(output_image)\n",
    "im.save(OUTPUT_FOLDER + '/' + 'output-' + str(time.time()) + '.jpg')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Copy of Copy of Untitled0.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
